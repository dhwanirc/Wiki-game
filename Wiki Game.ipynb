{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia game\n",
    "#### Dhwani Contractor\n",
    "\n",
    "## Main Function: findLinkRoute\n",
    "#### Input: \n",
    "startlink: Link of the starting page\n",
    "\n",
    "endlink: Link of the end page\n",
    "\n",
    "#### Output:\n",
    "result: Sequence of the page names\n",
    "\n",
    "## Other Functions:\n",
    "### getPageName\n",
    "#### Input: \n",
    "url: url of the link\n",
    "#### Output: \n",
    "pname: Pagename of the website\n",
    "\n",
    "### joinLink\n",
    "#### Input: \n",
    "pagename: Pagename of the link\n",
    "#### Output: \n",
    "url: The Wikipedia URL\n",
    "\n",
    "### findLinks\n",
    "#### Input: \n",
    "link: Link of the website \n",
    "#### Output: \n",
    "set :set of the pagename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splits the url annd takes the part comes after \"/wiki/\"\n",
    "def getPageName(url):\n",
    "    pname=url.split(\"/wiki/\",1)[1]\n",
    "    return pname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Joins the pagename with the wikipedia url\n",
    "def JoinLink(pagename): \n",
    "    return \"https://en.wikipedia.org/wiki/\"+pagename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find all the links available on the entire webpage and \n",
    "#puts it in a set so that if any link is there more than once, it won't be the part of set again\n",
    "\n",
    "def findLinks(link):\n",
    "    s=set()\n",
    "    resp = urllib.request.urlopen(link)\n",
    "    soup = BeautifulSoup(resp,\"lxml\", from_encoding=resp.info().get_param('charset'))\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        l=link['href']\n",
    "        if l.startswith('/wiki/') and l.find(\":\")==-1 and l.count('/') == 2:\n",
    "            pagename=getPageName(l)\n",
    "            s.add(pagename)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findLinkRoute(startlink,endlink):\n",
    "    #Find the pagename of the startlink and endlink\n",
    "    startPage=getPageName(startlink)\n",
    "    endPage=getPageName(endlink)\n",
    "    \n",
    "    #q is a Que of the pages we need to check\n",
    "    q = Queue()\n",
    "    #Parent is a directory which will keep track of the child(as key) and its parent(as value)\n",
    "    parent = {}\n",
    "    #Found is a flag which will be true when the endpage will match\n",
    "    found = False\n",
    "    #Making the first page's parent null\n",
    "    parent[startPage] = None\n",
    "    print(\"Finding path... Please wait\")\n",
    "    #Finds the link of all the pages on the start page\n",
    "    first_child=findLinks(startlink)\n",
    "    \n",
    "    #Put all the links in the queue \n",
    "    for i in first_child:\n",
    "        parent[i] = startPage\n",
    "        q.put(i)\n",
    "        if i == -1:\n",
    "            found = True\n",
    "                \n",
    "    #Searches the end page till the que is empty\n",
    "    while not q.empty() and not found:\n",
    "        #take the elements of the queue one by one\n",
    "        currentpage=q.get()\n",
    "        currentlink=JoinLink(currentpage)\n",
    "        #Find all the links on the current webpage\n",
    "        nextchild=findLinks(currentlink)\n",
    "        \n",
    "        #Save the parent page of the current page\n",
    "        for i in nextchild:\n",
    "            if i not in parent:\n",
    "                parent[i] = currentpage\n",
    "                q.put(i)\n",
    "                if i == endPage:\n",
    "                    #we found the link\n",
    "                    found = True\n",
    "                    break\n",
    "       \n",
    "    \n",
    "    print(\"The path is being found, creating a path now..\")\n",
    "    #build the path now\n",
    "    result = []\n",
    "    temp = endPage\n",
    "    #Trackback till the first page comes \n",
    "    while temp != startPage:\n",
    "        if(temp):\n",
    "            result.insert(0, parent[temp])\n",
    "            temp = parent[parent[temp]]\n",
    "        else:\n",
    "            break\n",
    "    result.insert(0, startPage) if result[0] != startPage else None\n",
    "    result.append(endPage)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding path... Please wait\n",
      "The path is being found, creating a path now..\n",
      "Web_Bot -> Taxation_in_Iran -> Tax_holiday\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "\n",
    "#Insert the start and end link here\n",
    "startlink=\"https://en.wikipedia.org/wiki/Web_Bot\"\n",
    "endlink=\"https://en.wikipedia.org/wiki/Tax_holiday\"\n",
    "path=findLinkRoute(startlink,endlink)\n",
    "\n",
    "#Print the path of the pages\n",
    "for page in path[:-1]:\n",
    "    print(page, sep=' ', end=' -> ', flush=True)\n",
    "print(path[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- Here, the applied algorithm finds the \"Shortest Path\" to reach the end page. Hence, the path is being completed just in 3 steps rather than the mentioned 4 steps path.\n",
    "\n",
    "\n",
    "- The time taken to find the result was around 20-25 min. This time can be improved by putting the depth limit. Means, the algorithm will match the end page name just till maximum depth. If the page is not found then the algorithm will start processing the other nodes. Drawback of such implementation will be if the end page is in the deeper node than the algorithm won't be able to find the solution. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
